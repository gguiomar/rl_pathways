{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplifying the environment\n",
    "* the basic idea is to turn this into a simple absorbing markov chain that can be scaled to any number of temporal states\n",
    "* there are 3 sets of states: pre-second tone states $s \\subset T_1$, post-second tone states $s \\subset T_2$ and a set of terminal states $s \\subset T_f$\n",
    "* there's a hold action that allows for the propagation of the agent through the states in $T_1$ and $T_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import matplotlib.gridspec as gridspec\n",
    "import copy as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class timing_task_csc():\n",
    "    \n",
    "    # things to implement:\n",
    "    # 1. automatic splitting of trial identities (left,right) for any number of second tones\n",
    "    \n",
    "    def __init__(self, n_states, second_tone_list, reward_magnitude, \n",
    "                 reward_probability, punishment_magnitude, punishment_probability):\n",
    "        \n",
    "        # Task variables\n",
    "        self.n_states = n_states # separate total number of transitions from total number of states\n",
    "        self.n_total_states = 2 * n_states + 5\n",
    "        self.n_actions = 3\n",
    "        \n",
    "        self.action_dict = np.arange(self.n_actions)\n",
    "        self.second_tone_list = second_tone_list\n",
    "        self.trial_types = len(self.second_tone_list)\n",
    "        self.n_trials = len(self.second_tone_list)\n",
    "        self.trials = np.zeros((self.n_trials, self.n_states*2, self.n_states))\n",
    "\n",
    "        \n",
    "        # Reward variables\n",
    "        self.reward_state = [0,0]\n",
    "        self.reward_magnitude = reward_magnitude\n",
    "        self.punishment_magnitude = punishment_magnitude\n",
    "        self.reward_probability = reward_probability\n",
    "        self.punishment_probability = punishment_probability\n",
    "\n",
    "        self.generate_environment()\n",
    "\n",
    "\n",
    "    def generate_environment(self):\n",
    "        self.generate_state_representation()\n",
    "        self.generate_state_sequence()\n",
    "        self.generate_environment_solution(0)\n",
    "    \n",
    "    def generate_state_representation(self):\n",
    "        \n",
    "        self.n_trials = len(self.second_tone_list)\n",
    "        tone1 = np.zeros((self.n_trials, self.n_states, self.n_states))\n",
    "        tone2 = np.zeros((self.n_trials, self.n_states, self.n_states))\n",
    "        self.trials = np.zeros((self.n_trials, self.n_states*2, self.n_states))\n",
    "\n",
    "        for k,e in enumerate(self.second_tone_list):\n",
    "            for i in range(self.n_states):\n",
    "                if i > self.n_states: # this limits the maximum second tone time\n",
    "                    tone1[k,i,i] = 0\n",
    "                    tone2[k,i,i] = 1\n",
    "                elif i < e:\n",
    "                    tone1[k,i,i] = 1\n",
    "                    tone2[k,i,i] = 0\n",
    "                else:\n",
    "                    tone1[k,i,i] = 0\n",
    "                    tone2[k,i,i] = 1\n",
    "                    \n",
    "            #self.trials.append(np.concatenate((tone1[k,:,:], tone2[k,:,:]), axis = 0))\n",
    "            self.trials[k,0:self.n_states,:] = tone1[k,:,:]\n",
    "            self.trials[k,self.n_states: 2*self.n_states,:] = tone2[k,:,:]\n",
    "        \n",
    "    def generate_state_sequence(self):\n",
    "        \n",
    "        # this function generates the state identities for the MDP\n",
    "        # that will be used in the simulation\n",
    "        \n",
    "        state = 0\n",
    "        self.trial_st = np.zeros((self.n_trials, self.n_states))\n",
    "        for k in range(self.n_trials):\n",
    "            cnt = 0\n",
    "            base = 0\n",
    "            for i,e in enumerate(self.trials[k].T):\n",
    "                state = np.nonzero(e)[0]\n",
    "                if i == 0:\n",
    "                    self.trial_st[k,i] = 0\n",
    "                if i != 0 and state != 0:\n",
    "                    self.trial_st[k,i] = state\n",
    "                if not state and i != 0:\n",
    "                    self.trial_st[k,i] = base + cnt\n",
    "                    cnt += 1\n",
    "        \n",
    "        self.trial_st = self.trial_st.astype(int)\n",
    "       \n",
    "    def plot_state_representation(self):\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        gs = gridspec.GridSpec(1, len(self.second_tone_list))\n",
    "        ax = []\n",
    "\n",
    "        #titles = ['short 1', 'short 2', 'long 1', 'long 2']\n",
    "        for k,e in enumerate(self.second_tone_list):\n",
    "            ax.append([])\n",
    "            ax[k] = fig.add_subplot(gs[0,k])\n",
    "            if k == 0: \n",
    "                ax[k].set_ylabel('State')\n",
    "            ax[k].set_xlabel('Time')\n",
    "            ax[k].imshow(self.trials[k], cmap='Greys')\n",
    "            #ax[k].set_title(titles[k])\n",
    "            \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def generate_environment_solution(self, plot_flag):\n",
    "        \n",
    "        # generates the policy for the optimal agent\n",
    "        \n",
    "        # map second tone to state identity\n",
    "        self.second_tone_state = np.zeros(self.n_trials)\n",
    "        for tr,st in enumerate(self.second_tone_list):\n",
    "            self.second_tone_state[tr] = np.nonzero(self.trials[tr][:,st])[0][0]\n",
    "\n",
    "        # map state identity to optimal action\n",
    "        self.opt_act = 2 * np.ones((self.n_trials, self.n_total_states))\n",
    "        for tr in range(self.n_trials):\n",
    "            for i,state in enumerate(self.trial_st[tr]):\n",
    "                if state >= self.second_tone_state[tr]:\n",
    "                    # split decisions in the middle\n",
    "                    if tr < self.n_trials/2: # short decision\n",
    "                        self.opt_act[tr,state] = 0\n",
    "                    else: # long decision\n",
    "                        self.opt_act[tr,state] = 1\n",
    "     \n",
    "        # take into account that the opt_act vector will have a set of 2's\n",
    "        # in the end because of the 0 states after the episode finishes\n",
    "        \n",
    "        if plot_flag:\n",
    "            plt.figure(figsize =(20,10))\n",
    "            plt.title('Optimal actions')\n",
    "            plt.imshow(self.opt_act)\n",
    "            plt.colorbar(fraction = 0.01)\n",
    "            plt.ylabel('trial type')\n",
    "            plt.xlabel('state')\n",
    "            plt.show()\n",
    "            \n",
    "    def test_environment_all(self):\n",
    "        \n",
    "        for tt in range(self.trial_types):\n",
    "            for cs in self.trial_st[tt]:\n",
    "                for a in range(self.n_actions):\n",
    "                    print([tt, cs, a], self.get_outcome(tt, cs, a))\n",
    "                    \n",
    "    def test_environment_action(self, action):\n",
    "        current_state = 0\n",
    "        next_state = 0\n",
    "        for i,tr in enumerate(second_tone_list):\n",
    "            for j,current_state in enumerate(env.trial_st[i]):\n",
    "                next_state, reward = env.get_outcome(i, current_state, action)\n",
    "                print([i, current_state, action, next_state, reward])\n",
    "    \n",
    "    def get_outcome(self, current_trial, current_state, action):\n",
    "        \n",
    "        next_state = 0\n",
    "        reward = 0\n",
    "        check_valid_state = np.argwhere(self.trial_st[current_trial] == current_state).shape[0] # should be > 0\n",
    "            \n",
    "        if check_valid_state: \n",
    "\n",
    "            # making a choice and going to terminal state\n",
    "            if action != 2:\n",
    "                #print('A')\n",
    "                state_index = np.argwhere(self.trial_st[current_trial] == current_state)[0][0]\n",
    "                \n",
    "                # if a decision is made before the terminal states\n",
    "                if current_state < self.n_total_states - 5: # buffer states \n",
    "                    #print('B')\n",
    "                    if action == self.opt_act[current_trial, current_state]:\n",
    "                        reward = self.reward_magnitude\n",
    "                        next_state = self.n_total_states - 5 # transition into terminal states\n",
    "                    else:                                                       \n",
    "                        reward = self.punishment_magnitude\n",
    "                        next_state = self.n_total_states - 5 # transition into terminal states\n",
    "                \n",
    "                # when we are in the terminal states\n",
    "                elif self.n_total_states - 5 <= current_state < self.n_total_states - 1:\n",
    "                    #print('C')\n",
    "                    reward = 0\n",
    "                    next_state = 0\n",
    "                \n",
    "                # when we reach the final terminal state we go back to the initial state\n",
    "                elif current_state == self.n_total_states:\n",
    "                    #print('D')\n",
    "                    reward = 0\n",
    "                    next_state = 0\n",
    "\n",
    "            # hold action and moving along the state space\n",
    "            else:\n",
    "                state_index = np.argwhere(self.trial_st[current_trial] == current_state)[0][0] #current state index\n",
    "\n",
    "                if state_index < np.argmax(self.trial_st[current_trial]):\n",
    "                    next_state = self.trial_st[current_trial][state_index + 1]\n",
    "                else:\n",
    "                    next_state = 0\n",
    "                    reward = 0\n",
    "        else:\n",
    "            next_state = 0\n",
    "            reward = 0\n",
    "\n",
    "        \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsteps = 30\n",
    "beta = 5\n",
    "gamma = 0.85\n",
    "rwd_mag = 10\n",
    "pun_mag = -5\n",
    "n_eps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 24\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  54 55 56 57 58 59]]\n",
      "--- 25\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 55 56 57 58 59]]\n",
      "--- 26\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 56 57 58 59]]\n",
      "--- 27\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 57 58 59]]\n",
      "--- 28\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 58 59]]\n",
      "--- 29\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "  24 25 26 27 28 59]]\n"
     ]
    }
   ],
   "source": [
    "for stl in np.arange(24,30):\n",
    "    print('---', stl)\n",
    "    env = timing_task_csc(tsteps, [stl], rwd_mag, 1, pun_mag, 1)\n",
    "    #env.test_environment_all()\n",
    "    print(env.trial_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
